import argparse
import json
import logging
import os
from glob import glob
from typing import Any, Dict, TypeVar

from scripts.cpe_match import MatchCriteriaIdGenerator
from scripts.normalization import (
    parse_cpe_5_version_info,
    normalize_collection_url,
    process_collection_url,
    generate_candidates,
    normalize_vendor,
    normalize,
)

MAX_BATCH_SIZE = 25

cve_enrichment_base_path = "cve-data-enrichment"
nvd_record_base_path = "national-vulnerability-database/data"
cve5_data_files = glob("cvelist-v5/cves/**/**/CVE-*.json")
generator = MatchCriteriaIdGenerator()
wordpress_plugin_name_to_slug = {}
wordpress_theme_name_to_slug = {}

with open("data/wordpress/plugins/by_name.json", "r") as f:
    wordpress_plugin_name_to_slug = json.load(f)

with open("data/wordpress/themes/by_name.json", "r") as f:
    wordpress_theme_name_to_slug = json.load(f)

wordpress_plugins = set(wordpress_plugin_name_to_slug.values())
wordpress_themes = set(wordpress_theme_name_to_slug.values())

only_allowed_assigners = True

allowed_assigners = {
    "adobe",
    "apache",
    "appcheck",
    "atlassian",
    #"canonical",
    "chrome",
    "cloudflare",
    "concretecms",
    "curl",
    "document fdn.",
    "eclipse",
    "elastic",
    "facebook",
    #"fedora",
    "gandc",
    "github_m",
    "github_p",
    "gitlab",
    "glibc",
    "go",
    "google",
    "govtech csg",
    "grafana",
    "hackerone",
    "hashicorp",
    #"ibm",
    "icscert",
    "isc",
    "jenkins",
    "jetbrains",
    "jfrog",
    "kubernetes",
    "libreswan",
    "microsoft",
    "mozilla",
    "nlnet labs",
    "openssl",
    "openvpn",
    "oracle",
    "ox",
    "patchstack",
    "php",
    "postgresql",
    "psf",
    #"redhat",
    "snyk",
    "tcpdump",
    "tenable",
    "tigera",
    "vulncheck",
    "wolfssl",
    "wordfence",
    "wpscan",
    "zabbix",
}

ignore_assigners = {
    # "vuldb",
    "fluid attacks",
    "incibe",
    "google_devices",
    "sap",
    "liferay",
    "google_android",
}

# There are some IDs that lack details, don't try to add them to the list of things to look at.
ignore_cves = {
    "CVE-2024-2397",  # no released versions of tcpdump were affected, only a range of commits in master
    "CVE-2023-3966",
    "CVE-2024-1062",
    "CVE-2024-1722",  # no upstream version info provided by RedHat
    "CVE-2024-1485",  # no upstream version info provided by RedHat
    "CVE-2024-1342",  # no upstream version info provided by RedHat
    "CVE-2024-1454",  # no upstream version info provided by RedHat
    "CVE-2024-1151",  # Linux kernel
    "CVE-2023-6681",  # no upstream version info provided by RedHat
    "CVE-2024-2045",  # session android app
    "CVE-2023-6132",  # aviva edge
    "CVE-2024-20325",  # cisco
    "CVE-2024-20294",  # cisco possibly incorrect cve data
    "CVE-2024-1394",  # go-fips thing corresponding to https://github.com/advisories/GHSA-78hx-gp6g-7mj6 - several packages and no known CPEs
    # TODO: These are KeyCloak ones reported by RedHat, but we don't know what the upstream fix is yet
    # The maven GitHub data for all of the KeyCloak vulns seems less than ideal, but annoyingly RedHat don't seem to
    # link the upstream commits in their Bugzilla, so need to figure out how to find these.
    "CVE-2024-1132",
    "CVE-2024-2419",
    "CVE-2024-1249",
    "CVE-2023-3597",
    "CVE-2023-6484",
    "CVE-2023-6544",
    "CVE-2023-6717",
    "CVE-2023-6787",
    # TODO: same situation for quarkus
    "CVE-2023-5675",
    # TODO: additional redhat stuff to sort out
    "CVE-2023-6596",
    "CVE-2024-1657",
    "CVE-2023-5685",
    "CVE-2024-2494",
    "CVE-2024-29895", # The actual versions are not specified here.  It seems this was possibly only between unreleased commits but needs further investigation.
    "CVE-2024-4438",
    "CVE-2024-4438",
    "CVE-2023-6725",
    "CVE-2023-6725",
    "CVE-2024-4436",
    "CVE-2024-4437",

    # Tenable plugin feed vulns: Unsure how to represent these currently as the versions reported in the vulns do not represent product versions
    "CVE-2024-2390",
    "CVE-2024-29205", # Ivanti Connect Secure weird version ranges
    "CVE-2024-21424", # Azure Compute Gallery
    "CVE-2024-30042", # Microsoft Office stuff -> no real versions on many of the records
}

# filter out stuff we don't care about right now
ignore_cpes = {
    "cpe:2.3:a:freebsd:freebsd:*:*:*:*:*:*:*:*",
    "cpe:2.3:a:microsoft:defender_for_endpoint:*:*:*:*:*:*:*:*",
    "cpe:2.3:a:miraheze:managewiki:*:*:*:*:*:*:*:*",  # only git repo commit info, no releases
    "cpe:2.3:a:cisco:unified_computing_system:*:*:*:*:*:*:*:*",
    "cpe:2.3:a:cisco:ucs_central_software:*:*:*:*:*:*:*:*",
    "cpe:2.3:a:cisco:ucs_manager:*:*:*:*:*:*:*:*",
    "cpe:2.3:a:ibm:cics_tx:*:*:*:*:advanced:*:*:*",
    "cpe:2.3:a:ibm:cics_tx:*:*:*:*:standard:*:*:*",
    "cpe:2.3:a:ibm:txseries_for_multiplatforms:*:*:*:*:*:*:*:*",
    "cpe:2.3:a:ibm:txseries_for_multiplatform:*:*:*:*:*:*:*:*",
}

ignore_vendors = {
    "netapp",
}

KeyType = TypeVar("KeyType")


def deep_update(
    mapping: Dict[KeyType, Any], *updating_mappings: Dict[KeyType, Any]
) -> Dict[KeyType, Any]:
    updated_mapping = mapping.copy()
    for updating_mapping in updating_mappings:
        for k, v in updating_mapping.items():
            if (
                k in updated_mapping
                and isinstance(updated_mapping[k], dict)
                and isinstance(v, dict)
            ):
                updated_mapping[k] = deep_update(updated_mapping[k], v)
            else:
                updated_mapping[k] = v
    return updated_mapping


class CPEPatternLookup:
    def __init__(
        self,
        curated_files: list[str],
        generated_files: list[str],
        logger: logging.Logger = None,
    ):
        if not logger:
            logger = logging.getLogger(self.__class__.__name__)
        self.logger = logger
        self.cpe_types = ["application", "os", "hardware"]
        self.__lookup_by_collection_url_and_package_name__: dict[
            str, dict[str, dict[str, list[str]]]
        ] = {}
        self.__lookup_by_vendor_and_product__: dict[
            str, dict[str, dict[str, list[str]]]
        ] = {}
        self.__lookup_by_product__: dict[str, dict[str, list[str]]] = {}
        self.__load_data__(curated_files, generated_files)

    def __load_data__(self, curated_files: list[str], generated_files: list[str]):
        if not curated_files:
            curated_files = []

        if not generated_files:
            generated_files = []

        for t in self.cpe_types:
            if t not in self.__lookup_by_collection_url_and_package_name__:
                self.__lookup_by_collection_url_and_package_name__[t] = {}
            if t not in self.__lookup_by_vendor_and_product__:
                self.__lookup_by_vendor_and_product__[t] = {}
            if t not in self.__lookup_by_product__:
                self.__lookup_by_product__[t] = {}

        for file in generated_files + curated_files:
            if os.path.exists(file):
                self.logger.debug(f"loading CPE mapping file {file}")
                cpe_type = None
                if file.endswith("/application.json"):
                    cpe_type = "application"
                elif file.endswith("/os.json"):
                    cpe_type = "os"
                elif file.endswith("/hardware.json"):
                    cpe_type = "hardware"
                else:
                    self.logger.warning(
                        f"skipping loading CPE mapping {file} because the type is not recognized"
                    )
                    continue

                with open(file) as fp:
                    if "/by_collection_url_and_package_name/" in file:
                        self.__lookup_by_collection_url_and_package_name__[
                            cpe_type
                        ] = deep_update(
                            self.__lookup_by_collection_url_and_package_name__[
                                cpe_type
                            ],
                            json.load(fp),
                        )
                    elif "/by_vendor_and_product/" in file:
                        self.__lookup_by_vendor_and_product__[cpe_type] = deep_update(
                            self.__lookup_by_vendor_and_product__[cpe_type],
                            json.load(fp),
                        )
                    elif "/by_product/" in file:
                        self.__lookup_by_product__[cpe_type] = deep_update(
                            self.__lookup_by_product__[cpe_type], json.load(fp)
                        )

    def lookup(
        self,
        collection_url: str | None,
        package_name: str | None,
        vendor: str | None,
        product: str | None,
        cpe_types: list[str] | None = None,
    ) -> list[str] | None:
        if not cpe_types:
            cpe_types = self.cpe_types

        collection_url = normalize_collection_url(collection_url)
        package_names = generate_candidates(normalize(package_name))
        vendors = generate_candidates(normalize_vendor(vendor))
        products = generate_candidates(normalize(product))

        if not package_names:
            package_names = products

        if collection_url and package_names:
            for p in package_names:
                cpes = set()
                for t in cpe_types:
                    type_cpes = (
                        self.__lookup_by_collection_url_and_package_name__[t]
                        .get(collection_url, {})
                        .get(p)
                    )
                    if type_cpes:
                        cpes.update(type_cpes)
                if cpes:
                    return cpes

        if vendors and products:
            for v in vendors:
                for p in products:
                    cpes = set()
                    for t in cpe_types:
                        type_cpes = (
                            self.__lookup_by_vendor_and_product__[t].get(v, {}).get(p)
                        )
                        if type_cpes:
                            cpes.update(type_cpes)
                    if cpes:
                        return cpes

        if products:
            for p in products:
                cpes = set()
                for t in cpe_types:
                    type_cpes = self.__lookup_by_product__[t].get(p)
                    if type_cpes:
                        cpes.update(type_cpes)
                if cpes:
                    return cpes

        return None


curated_files = glob("data/cpe/curated/lookup/**/*.json", recursive=True)
generated_files = glob("data/cpe/generated/lookup/**/*.json", recursive=True)

cpe_lookup = CPEPatternLookup(curated_files, generated_files)
cpe_lookup.cpe_types = ["application"]


def get_existing_nvd_record(cve_id: str) -> dict[str, Any] | None:
    """
    Load an NVD record from the nvd_record_base_path location.

    Args:
        cve_id (str): A string containing a valid CVE ID

    Returns:
        dict: The parsed JSON for the NVD record

    By default the nvd_record_base_path location is
    ./national-vulnerability-database/data
    """
    cve_id = cve_id.upper()
    year = cve_id.split("-")[1]
    nvd_record_path = os.path.join(nvd_record_base_path, year, f"{cve_id}.json")
    if not os.path.exists(nvd_record_path):
        logging.debug(f"No NVD record for {cve_id}")
        return None

    with open(nvd_record_path, "r") as f:
        logging.debug(f"Loading current NVD record for {cve_id}")
        return json.load(f)


def get_existing_anchore_enrichment_record(cve_id: str) -> dict[str, Any] | None:
    """
    Load an existing anchore cve enrichment record from the cve-data-enrichment directory

    Args:
        cve_id (str): A string containing a valid CVE ID

    Returns:
        dict: The parsed JSON for the enrichment record

    By default the cve_enrichment_base_path location is
    ./cve-data-enrichment
    """
    cve_id = cve_id.upper()
    year = cve_id.split("-")[1]
    record_path = os.path.join(
        cve_enrichment_base_path, "data/anchore", year, f"{cve_id}.json"
    )
    if not os.path.exists(record_path):
        logging.debug(f"No Anchore CVE enrichment record for {cve_id}")
        return None

    with open(record_path, "r") as f:
        logging.debug(f"Loading existing Anchore CVE enrichment record for {cve_id}")
        return json.load(f)


def persist_anchore_enrichment_record(
    cve_id: str,
    assigner: str,
    affected: dict,
    reason: str = None,
    needs_review: bool = True,
    refs: list[str] = None,
    solutions: list[str] = None,
):
    """
    Save anchore enriched record to the proper location in the cve-data-enrichment directory

    Args:
        cve_id (str): A string containing a valid CVE ID
        assigner (str): The CNA that assigned the CVE 
        affected (dict): The CVE5 format affected records
        reason (str): A reason indicating why the enrichment is needed
        needs_review (bool): Whether or not the record needs human review before being committed

    Returns:
        Nothing

    By default the cve_enrichment_base_path location is
    ./cve-data-enrichment
    """

    if not reason:
        reason = "Added CPE configurations because not yet analyzed by NVD."
    cve_id = cve_id.upper()
    logging.info(f"Persisting Anchore enrichment record for {cve_id}")
    record = {
        "additionalMetadata": {
            "cveId": cve_id,
            "cna": assigner,
            "reason": reason,
        },
        "adp": {
            "providerMetadata": {
                "orgId": "00000000-0000-4000-8000-000000000000",
                "shortName": "anchoreadp",
            },
            "affected": affected,
        },
    }

    if needs_review:
        record["additionalMetadata"]["needsReview"] = needs_review

    if refs:
        record["additionalMetadata"]["references"] = refs

    if solutions:
        record["additionalMetadata"]["solutions"] = solutions

    year = cve_id.split("-")[1]
    persist_dir = os.path.join(cve_enrichment_base_path, "data/anchore", year)
    if not os.path.exists(persist_dir):
        os.makedirs(persist_dir, exist_ok=True)

    with open(os.path.join(persist_dir, f"{cve_id}.json"), "w") as fp:
        json.dump(record, fp, indent=2, sort_keys=True, ensure_ascii=False)


def process(cve5_record: dict[str, Any]) -> bool:
    if not cve5_record:
        return False

    # Try to grab the CVE ID from the JSON record
    cve_id = cve5_record.get("cveMetadata", {}).get("cveId")

    if not cve_id:
        logging.warning("unable to parse cve id for CVE 5 record")
        return False

    # Ignore IDs without enough data
    if cve_id in ignore_cves:
        return False

    # If an enrichment record exists, load its JSON
    anchore_enrichment_record = get_existing_anchore_enrichment_record(cve_id)
    rejected_date = cve5_record.get("cveMetadata", {}).get("dateRejected")
    cna_node = cve5_record.get("containers", {}).get("cna", {})

    # If a CVE is rejected skip it
    if rejected_date:
        logging.debug(
            f"skipping CVE 5 record for {cve_id} because it has been rejected"
        )
        return False

    assigner = cve5_record.get("cveMetadata", {}).get("assignerShortName")
    if assigner:
        assigner = assigner.lower()

    # Ignore certain assigners
    if assigner and assigner in ignore_assigners:
        logging.debug(
            f"skipping CVE 5 record for {cve_id} because assigner {assigner} is in the ignore list"
        )
        return False

    # We have an approved list of assigners, we can turn this off if we want, but to start we want to focus on on assigners we know provide good data
    if only_allowed_assigners:
        if not assigner or assigner not in allowed_assigners:
            logging.debug(
                f"skipping CVE 5 record for {cve_id} because assigner {assigner} is not in the automation allowlist"
            )
            return False

    logging.debug(f"Processing CVE 5 record for {cve_id}")

    # Load the current NVD JSON
    nvd_record = get_existing_nvd_record(cve_id)
    # TODO: We need to enrich records even when NVD has nothing
    # not currently in scope to create records that don't yet exist at all in NVD,
    # though could be at a later date if NVD stops working entirely
    if not nvd_record or "cve" not in nvd_record:
        logging.warning(
            f"No NVD record currently exists for {cve_id}.  Have you pulled in latest"
        )
        return False

    # Skip the IDs that have already been through NVD analysis
    if nvd_record["cve"]["vulnStatus"] not in {"Awaiting Analysis", "Received"}:
        return False

    # TODO: reconcile?
    if anchore_enrichment_record:
        logging.debug(
            f"There is already an existing anchore enrichment record for {cve_id}"
        )
        return False

    # Skip IDs that already have configurations. The data can have issues, it's possible an ID could exist
    # that's not set to analyzed but also has configurations
    nvd_configs = nvd_record["cve"].get("configurations")
    if nvd_configs:
        logging.debug(f"CPE configurations already present on NVD record for {cve_id}")
        return False

    # This is where we collect what we need to construct a CPE
    # We use solutions to attempt to parse fix version when none is provided in the versions array
    solutions = set()
    for s in cna_node.get("solutions", []):
        solution = s.get("value")
        if solution:
            solutions.add(solution)

    affected_records = []

    for affected in cna_node.get("affected", []):
        collection_url = affected.get("collectionURL")
        package_name = affected.get("packageName")
        vendor = affected.get("vendor")
        product = affected.get("product")
        repo = affected.get("repo")
        modules = affected.get("modules")
        platforms = affected.get("platforms")

        collection_url, package_name = process_collection_url(
            collection_url, package_name
        )
        if not repo and collection_url and package_name:
            if collection_url == "wordpress.org/plugins":
                repo = f"https://plugins.svn.wordpress.org/{package_name}"
            elif collection_url == "wordpress.org/themes":
                repo = f"https://themes.svn.wordpress.org/{package_name}"

        if not collection_url:
            if assigner in {"wpscan", "patchstack", "wordfence"}:
                normalized_product = normalize(product)
                normalized_package_name = normalize(package_name)

                if normalized_product and not normalized_package_name:
                    if normalized_product in wordpress_plugin_name_to_slug:
                        package_name = wordpress_plugin_name_to_slug[normalized_product]
                        collection_url = "wordpress.org/plugins"
                    elif normalized_product in wordpress_theme_name_to_slug:
                        package_name = wordpress_theme_name_to_slug[normalized_product]
                        collection_url = "wordpress.org/themes"
                elif normalized_package_name:
                    if normalized_package_name in wordpress_plugins:
                        collection_url = "wordpress.org/plugins"
                    elif normalized_package_name in wordpress_themes:
                        "wordpress.org/themes"
            elif assigner == "github_m" and vendor and product:
                collection_url = "github.com"
                package_name = f"{vendor.lower()}/{product.lower()}"
                if not repo:
                    repo = f"https://github.com/{package_name}"

        cpes = set()
        versioned_cna_cpes = set()
        normalized_vendor = normalize_vendor(vendor)
        normalized_product = normalize(product)

        if assigner == "microsoft" and (
            # These are too complicated version-wise to bother with for now (they often have platform-dependent version constraints)
            normalized_product.startswith("windows ") or
            normalized_product.startswith("microsoft office ") or
            normalized_product.startswith("microsoft 365 ") or
            normalized_product.startswith("microsoft excel ") or
            normalized_product.startswith("excel ") or
            normalized_product.startswith("azure ai search") or
            normalized_product.startswith("microsoft .net framework") or
            normalized_product.startswith("microsoft sql server") or
            normalized_product.startswith("microsoft ole db driver") or
            normalized_product.startswith("microsoft odbc driver")
        ):
            logging.debug(f"Skipping some microsoft product on {cve_id} for now: {normalized_product}")
            continue

        if normalized_vendor and "oracle" in normalized_vendor:
            # Java is special
            if normalized_product and "java" in normalized_product:
                logging.warning(f"{cve_id} for java stuff requires manual review")
                return False

        # if normalize_vendor(vendor) not in ["go standard library", "google.golang.org/protobuf"]:
        #    continue

        for cpe in affected.get("cpes", []):
            if cpe.startswith("cpe:2.3"):
                components = cpe.split(":")
                if len(components) == 13:
                    if components[5] not in {"*", "-"}:
                        versioned_cna_cpes.add(cpe)
                    components[5] = "*"
                    components[6] = "*"
                    
                    if assigner == "microsoft" and components[4] == "visual_studio":
                        if normalized_product.startswith("visual studio 2022") or normalized_product.startswith("microsoft visual studio 2022"):
                            cpes.add("cpe:2.3:a:microsoft:visual_studio_2022:*:*:*:*:*:*:*:*")
                        if normalized_product.startswith("visual studio 2017") or normalized_product.startswith("microsoft visual studio 2017"):
                            cpes.add("cpe:2.3:a:microsoft:visual_studio_2017:*:*:*:*:*:*:*:*")
                        if normalized_product.startswith("visual studio 2019") or normalized_product.startswith("microsoft visual studio 2019"):
                            cpes.add("cpe:2.3:a:microsoft:visual_studio_2019:*:*:*:*:*:*:*:*")
                        
                    
                    logging.debug(
                        f"Adding CPE {cpe!r} provided by the CNA on the CVE record for {cve_id!r}"
                    )
                    cpes.add(":".join(components))

        lookup_cpes = cpe_lookup.lookup(
            collection_url=collection_url,
            package_name=package_name,
            vendor=vendor,
            product=product,
        )
        if lookup_cpes:
            logging.debug(f"Discovered CPES {lookup_cpes!r} via lookups for {cve_id!r}")
            cpes.update(lookup_cpes)

        for cpe in ignore_cpes:
            if cpe in cpes:
                cpes.remove(cpe)

        if not cpes:
            # if not collection_url and not package_name and assigner in {"wordfence"}:
            #     for r in cna_node.get("references", []):
            #         url = r.get("url")
            #         if url and url.startswith("https://plugins.trac.wordpress.org/"):
            #             package_name = url.split("/")[-1]
            #             collection_url = "wordpress.org/plugins"
            #             break
            #         if url and url.startswith("https://themes.trac.wordpress.org/"):
            #             package_name = url.split("/")[-1]
            #             collection_url = "wordpress.org/themes"
            #             break
            #     if package_name and collection_url:
            #         lookup_cpes = cpe_lookup.lookup(collection_url=collection_url, package_name=package_name, vendor=vendor, product=product)
            #         if lookup_cpes:
            #             logging.debug(f"Discovered CPES {lookup_cpes!r} via lookups for {cve_id!r}")
            #             cpes.update(lookup_cpes)

            # cpes = cpes_from_vendor_and_product(vendor=vendor, product=product)
            # logging.debug(f"Generated CPES {cpes!r} from vendor={vendor!r}, product={product!r} for {cve_id!r}")
            if not cpes:
                # TODO: create some sort of generator if the values aren't all equivalent to empty
                # for now just bail
                logging.warning(
                    f"No CPEs discovered or generated for affected entry: {affected!r} on {cve_id!r}"
                )
                continue

        # Possible status values are `affected`, `unaffected`, and `unknown`
        # should be considered `unknown` if not specified
        default_status = affected.get("defaultStatus", "unknown").lower()
        enriched_versions = []
        versions = affected.get("versions", [])

        for v in versions:
            enriched_version = {}
            status = v.get("status", default_status).lower()
            (
                version,
                less_than,
                less_than_or_equal,
                version_type,
            ) = parse_cpe_5_version_info(v, solutions)

            if not version_type:
                version_type = "custom"

            # Correct for RedHat versioning
            if (
                (assigner == "redhat" or assigner == "fedora")
                and status == "unaffected"
                and normalize_vendor(vendor) is None
                and normalize(product) is not None
            ):
                if version and not less_than and not less_than_or_equal:
                    status = "affected"
                    v["status"] = status
                    v["lessThan"] = version
                    v["version"] = None
                    (
                        version,
                        less_than,
                        less_than_or_equal,
                        version_type,
                    ) = parse_cpe_5_version_info(v, solutions)

            # TODO: Turn versions with `,` into a list of versions
            if version and "," in version:
                logging.warning(f"Skipping version {version} for {cve_id!r}")
                continue

            if version_type == "git" and assigner not in ["concretecms", "zabbix"]:
                logging.warning(f"Skipping git version type for {cve_id!r}")
                continue

            if less_than:
                enriched_version["lessThan"] = less_than
                if version and version not in {"*"}:
                    enriched_version["version"] = version
                else:
                    enriched_version["version"] = "0"
            elif less_than_or_equal:
                enriched_version["lessThanOrEqual"] = less_than_or_equal
                if version and version not in {"*"}:
                    enriched_version["version"] = version
                else:
                    enriched_version["version"] = "0"
            elif version:
                enriched_version["version"] = version
            else:
                logging.warning(
                    f"no useable version information extracted for affected entry: {affected!r}, version: {v!r} on {cve_id!r}"
                )
                break

            if assigner == "wolfssl" and version_type == "release bundle":
                version_type = "semver"

            enriched_version["versionType"] = version_type
            enriched_version["status"] = status
            enriched_versions.append(enriched_version)

        if not enriched_versions:
            continue

        affected_entry = {}
        if collection_url:
            affected_entry["collectionURL"] = f"https://{collection_url.strip()}"
        if package_name:
            affected_entry["packageName"] = package_name.strip()
        if vendor and vendor not in {"Unknown"}:
            affected_entry["vendor"] = vendor.strip()
        if product and product not in {"Unknown"}:
            affected_entry["product"] = product.strip()
        if repo:
            affected_entry["repo"] = repo.strip()
        if modules:
            affected_entry["modules"] = sorted(modules)
        if platforms:
            p = []
            for platform in platforms:
                if platform.lower() != "unknown":
                    p.append(platform)
            if p:
                affected_entry["platforms"] = sorted(p)

        affected_entry["cpes"] = sorted(list(cpes))
        affected_entry["versions"] = enriched_versions
        affected_records.append(affected_entry)

    if affected_records:
        logging.info(
            f"Creating Anchore enriched record for {cve_id!r} with affected section: {affected_records!r}"
        )
        refs = set()
        for r in cna_node.get("references", []):
            url = r.get("url")
            if url:
                refs.add(url)

        if refs:
            refs = sorted(list(refs))

        if solutions:
            solutions = list(solutions)

        needs_review = True
        if assigner in {"mozilla", "chrome", "jetbrains", "patchstack"}:
            needs_review = False

        persist_anchore_enrichment_record(cve_id, assigner, affected_records, needs_review=needs_review, refs=refs, solutions=solutions)
        return True
    else:
        message = f"Unable to automatically determine CPE config for {cve_id} from assigner {assigner}: vendor={vendor}, product={product}, collection_url={collection_url}, package_name={package_name}"
        if assigner in ["wordfence", "patchstack", "wpscan", "github_m"]:
            logging.debug(message)
        else:
            logging.warning(message)

    return False


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('-v', '--verbose', action='count', default=0)
    parser.add_argument('-a','--assigners', nargs='+', help='Specific assigners (CNAs) to process records for', required=False)
    parser.add_argument('-b','--batch-size', help='Maximum update batch size', required=False, type=int, default=MAX_BATCH_SIZE)
    args = parser.parse_args()

    log_level = "INFO"
    if args.verbose > 0:
        log_level = "DEBUG"

    if args.assigners:
        allowed_assigners = set(args.assigners)

    MAX_BATCH_SIZE = args.batch_size

    created = 0
    logging.basicConfig(level=log_level)

    # Loop over all the CVE json files from cvelist-v5
    for cve5_file in cve5_data_files:
        with open(cve5_file) as fp:
            cve5_record = json.load(fp)

        created_override = process(cve5_record)

        if created_override:
            created += 1

        # We only run as many time as defined by the batch size variable to avoid overwhelming whoever will review the findings
        if created >= MAX_BATCH_SIZE:
            logging.warning("stopped processing as maximum created records reached")
            break
