import json
import logging
import requests

from glob import glob

from scripts.normalization import normalize_collection_url

url_exists = {}

excluded_cves = {
    "CVE-2024-1139",
}
excluded_cpes = {
    "cpe:2.3:a:redhat:openshift_container_platform:*:*:*:*:*:*:*:*",
    "cpe:2.3:a:redhat:openshift:*:*:*:*:*:*:*:*",
}
lookup_by_cpe = {}

excluded_collection_urls = {
    "access.redhat.com",
    "packages.fedoraproject.org",
    "packages.fedoraproject.org",
}

excluded_github_users = {
    "zhuoniba/",
    "wp-plugins/",
    "wp-themes/",
    "advisories/",
    "beatriz-ai-boop/",
    "navaidzansari/",
    "nickguitar/",
}

def populate_lookup(filename: str):
    with open(filename, "r") as f:
        for collection_url, packages in json.load(f).items():
            excluded = False
            for u in excluded_collection_urls:
                if collection_url.startswith(u):
                    excluded = True
                    break

            if excluded:
                continue

            for package_name, cpes in packages.items():
                if collection_url.startswith("github.com"):
                    for u in excluded_github_users:
                        if package_name.startswith(u):
                            excluded = True
                            break

                if excluded:
                    continue

                for cpe in cpes:
                    if cpe in excluded_cpes:
                        continue

                    if cpe not in lookup_by_cpe:
                        lookup_by_cpe[cpe] = {}

                    if collection_url.startswith("wordpress.org"):
                        #if " " in package_name:
                        #    continue
                        # TODO: Figure out wordpress plugin issues
                        continue
                        # package_name = package_name.replace(" ", "-")

                        # url = f"https://{collection_url}/{package_name}"

                        # if url not in url_exists:
                        #     logging.warning(url)
                        #     url_exists[url] = requests.head(url).status_code == 200

                        # if not url_exists[url]:
                        #     continue

                    if collection_url not in lookup_by_cpe[cpe]:
                        lookup_by_cpe[cpe][collection_url] = set()

                    lookup_by_cpe[cpe][collection_url].add(package_name)


# populate_lookup(
#     "data/cpe/generated/lookup/by_collection_url_and_package_name/application.json"
# )
populate_lookup(
    "data/cpe/curated/lookup/by_collection_url_and_package_name/application.json"
)


def determine_additional_metadata_from_cpes(
    cve_id: str, collection_url: str, package_name: str, repo: str, cpes: list[str]
):
    collection_url_info = {}

    for cpe in affected["cpes"]:
        metadata = lookup_by_cpe.get(cpe)

        if not metadata:
            continue

        for collection_url, packages in metadata.items():
            if collection_url not in collection_url_info:
                collection_url_info[collection_url] = set()

            collection_url_info[collection_url].update(packages)

    if (
        "github.com" in collection_url_info
        and len(collection_url_info["github.com"]) == 1
    ):
        if not repo:
            p = list(collection_url_info["github.com"])[0]
            repo = f"https://github.com/{p}"
        del collection_url_info["github.com"]

    if len(collection_url_info) > 1:
        logging.warning(
            f"Multiple collectionURL possibilities for {cve_id}.  Please review manually: {collection_url_info}"
        )
        return collection_url, package_name, repo

    if len(collection_url_info) == 1:
        if len(collection_url_info.values()) > 1:
            logging.warning(
                f"Multiple packageName possibilities for {cve_id}.  Please review manually: {list(collection_url_info.values())}"
            )
            return collection_url, package_name, repo

        collection_url = list(collection_url_info.keys())[0]
        package_name = list(list(collection_url_info.values())[0])[0]

        if not repo and collection_url and package_name:
            if collection_url == "wordpress.org/plugins":
                repo = f"https://plugins.svn.wordpress.org/{package_name}"
            elif collection_url == "wordpress.org/themes":
                repo = f"https://themes.svn.wordpress.org/{package_name}"

    if collection_url and not repo and (collection_url.startswith("github.com") or collection_url.startswith("gitlab.com")):
        repo = f"https://{collection_url}"
        collection_url = None

    if collection_url == "github.com":
        collection_url = None
        package_name = None

    return collection_url, package_name, repo


for enriched_path in glob(
    "cve-data-enrichment/data/anchore/**/CVE-*.json", recursive=True
):
    with open(enriched_path) as f:
        enriched = json.load(f)

    cve_id = enriched["additionalMetadata"]["cveId"]

    if cve_id in excluded_cves:
        continue

    for affected in enriched["adp"]["affected"]:
        collection_url = normalize_collection_url(affected.get("collectionURL"))
        package_name = affected.get("packageName")
        repo = affected.get("repo")

        if collection_url == "github.com" and package_name and not repo:
            repo = f"https://github.com/{package_name}"

        if not collection_url or collection_url == "github.com":
            collection_url, package_name, repo = (
                determine_additional_metadata_from_cpes(
                    cve_id, collection_url, package_name, repo, affected["cpes"]
                )
            )

        if collection_url:
            if not collection_url.startswith("https://"):
                collection_url = f"https://{collection_url}"

        if collection_url == repo:
            collection_url = None

        if collection_url:
            affected["collectionURL"] = collection_url

        if package_name:
            affected["packageName"] = package_name

        if repo:
            affected["repo"] = repo

    with open(enriched_path, "w") as f:
        json.dump(enriched, f, ensure_ascii=False, sort_keys=True, indent=2)
